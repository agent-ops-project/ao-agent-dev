# Agent examples

We implement example workflows here, including solutions for benchmarks.
  
## Simple

 - `debug_examples`: Simple workflows to debug our code.

 - `doc_bench`: Questions over PDFs.

 - `human-eval`: Evaluate model-generated code. Download data from https://github.com/openai/human-eval.

## Medium

 - TODO

## Complex

 - `DeepResearch`: MiroFlow open-source deep research agent.

 - `SWE-bench`: SWE-bench benchmark with our own agent created by Claude code.